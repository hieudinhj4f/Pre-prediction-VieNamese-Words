import os
import collections
import pickle
import re
from underthesea import word_tokenize
from underthesea.dictionary import Dictionary

class VietnamesePredictor:
    def __init__(self):
        self.unigram_counts = collections.Counter()
        self.bigram_counts = collections.Counter()
        self.trigram_counts = collections.Counter()

    def clean_text(self, text):
        text = text.lower().strip()
        text = re.sub(r'[^\w\s]', '', text)
        return text

    def train_file(self, file_path):
        """Äá»c vÃ  há»c tá»« tá»‡p vÄƒn báº£n"""
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                text = self.clean_text(line)
                if not text: continue
                # Tokenize tiáº¿ng Viá»‡t
                tokens = word_tokenize(text, format="text").split()
                
                self.unigram_counts.update(tokens)
                if len(tokens) >= 2:
                    self.bigram_counts.update(zip(tokens, tokens[1:]))
                if len(tokens) >= 3:
                    self.trigram_counts.update(zip(tokens, tokens[1:], tokens[2:]))

    def inject_internal_dictionary(self):
        """Náº¡p tá»« Ä‘iá»ƒn ná»™i bá»™ cá»§a underthesea (Sá»­a lá»—i Singleton)"""
        print("--- ğŸ“š Äang náº¡p tá»« Ä‘iá»ƒn ná»™i bá»™ tá»« Underthesea ---")
        try:
            # CÃ¡ch gá»i Ä‘Ãºng cho Singleton trong underthesea
            dic = Dictionary.instance()
            words = dic.words
            for w in words:
                word = w.lower()
                if word not in self.unigram_counts:
                    self.unigram_counts[word] = 1
        except Exception as e:
            print(f"âš ï¸ KhÃ´ng thá»ƒ náº¡p tá»« Ä‘iá»ƒn ná»™i bá»™: {e}")

    def save_model(self, path):
        with open(path, 'wb') as f:
            pickle.dump(self, f)
        print(f"âœ… ÄÃ£ lÆ°u mÃ´ hÃ¬nh táº¡i: {path}")

    @staticmethod
    def load_model(path):
        with open(path, 'rb') as f:
            return pickle.load(f)

    def predict(self, context_tuple, prefix, top_n=5):
        w1, w2 = [w.lower() for w in context_tuple]
        pre = prefix.lower()
        scores = collections.defaultdict(float)

        # Trigram (Trá»ng sá»‘ 10)
        for (tw1, tw2, tw3), count in self.trigram_counts.items():
            if tw1 == w1 and tw2 == w2 and tw3.startswith(pre):
                scores[tw3] += count * 10
        # Bigram (Trá»ng sá»‘ 1)
        if len(scores) < top_n:
            for (bw1, bw2), count in self.bigram_counts.items():
                if bw1 == w2 and bw2.startswith(pre):
                    scores[bw2] += count
        # Unigram (Trá»ng sá»‘ 0.01)
        if len(scores) < top_n:
            for word, count in self.unigram_counts.items():
                if word.startswith(pre) and word not in scores:
                    scores[word] += count * 0.01

        results = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return [word.replace("_", " ") for word, _ in results[:top_n]]
    

    def update_learning(self, sentence):
        """HÃ m giÃºp mÃ´ hÃ¬nh há»c thÃªm tá»« cÃ¢u ngÆ°á»i dÃ¹ng vá»«a nháº­p"""
        text = self.clean_text(sentence)
        if not text: return
        
        tokens = word_tokenize(text, format="text").split()
        
        # Cáº­p nháº­t cÃ¡c bá»™ Ä‘áº¿m ngay láº­p tá»©c
        self.unigram_counts.update(tokens)
        if len(tokens) >= 2:
            self.bigram_counts.update(zip(tokens, tokens[1:]))
        if len(tokens) >= 3:
            self.trigram_counts.update(zip(tokens, tokens[1:], tokens[2:]))
        
        # LÆ°u láº¡i mÃ´ hÃ¬nh ngay Ä‘á»ƒ "ghi nhá»›" vÄ©nh viá»…n
        # (LÆ°u Ã½: Trong thá»±c táº¿ náº¿u dá»¯ liá»‡u lá»›n thÃ¬ nÃªn lÆ°u Ä‘á»‹nh ká»³ Ä‘á»ƒ trÃ¡nh cháº­m mÃ¡y)
        # self.save_model("vietnamese_ngram_mega.pkl")